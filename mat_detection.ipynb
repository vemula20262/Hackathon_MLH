{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cc1f7077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found: ['aerosol_cans', 'aluminum_food_cans', 'aluminum_soda_cans', 'cardboard_boxes', 'cardboard_packaging', 'clothing', 'coffee_grounds', 'disposable_plastic_cutlery', 'eggshells', 'food_waste', 'glass_beverage_bottles', 'glass_cosmetic_containers', 'glass_food_jars', 'magazines', 'newspaper', 'office_paper', 'paper_cups', 'plastic_cup_lids', 'plastic_detergent_bottles', 'plastic_food_containers', 'plastic_shopping_bags', 'plastic_soda_bottles', 'plastic_straws', 'plastic_trash_bags', 'plastic_water_bottles', 'shoes', 'steel_food_cans', 'styrofoam_cups', 'styrofoam_food_containers', 'tea_bags']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os, random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "\n",
    "# %%\n",
    "# USER: change this to your dataset folder (same as your PyTorch DATA_DIR)\n",
    "DATA_DIR = Path(\"/Users/vaibav/Downloads/Material_Dataset/images/images\")\n",
    "\n",
    "if not DATA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"{DATA_DIR} does not exist — update the path.\")\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Build list of filepaths + labels (sorted classes to match ImageFolder behavior)\n",
    "class_names = sorted([p.name for p in DATA_DIR.iterdir() if p.is_dir()])\n",
    "print(\"Classes found:\", class_names)\n",
    "num_classes = len(class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bbc5cb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 classes found.\n"
     ]
    }
   ],
   "source": [
    "print(num_classes, \"classes found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "337f2b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/aerosol_cans/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/aerosol_cans/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/aluminum_food_cans/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/aluminum_food_cans/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/aluminum_soda_cans/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/aluminum_soda_cans/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/cardboard_boxes/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/cardboard_boxes/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/cardboard_packaging/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/cardboard_packaging/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/clothing/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/clothing/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/coffee_grounds/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/coffee_grounds/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/disposable_plastic_cutlery/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/disposable_plastic_cutlery/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/eggshells/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/eggshells/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/food_waste/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/food_waste/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/glass_beverage_bottles/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/glass_beverage_bottles/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/glass_cosmetic_containers/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/glass_cosmetic_containers/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/glass_food_jars/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/glass_food_jars/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/magazines/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/magazines/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/newspaper/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/newspaper/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/office_paper/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/office_paper/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/paper_cups/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/paper_cups/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/plastic_cup_lids/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/plastic_cup_lids/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/plastic_detergent_bottles/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/plastic_detergent_bottles/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/plastic_food_containers/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/plastic_food_containers/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/plastic_shopping_bags/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/plastic_shopping_bags/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/plastic_soda_bottles/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/plastic_soda_bottles/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/plastic_straws/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/plastic_straws/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/plastic_trash_bags/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/plastic_trash_bags/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/plastic_water_bottles/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/plastic_water_bottles/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/shoes/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/shoes/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/steel_food_cans/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/steel_food_cans/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/styrofoam_cups/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/styrofoam_cups/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/styrofoam_food_containers/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/styrofoam_food_containers/real_world\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/tea_bags/default\n",
      "Looking into: /Users/vaibav/Downloads/Material_Dataset/images/images/tea_bags/real_world\n",
      "Total images: 15000\n"
     ]
    }
   ],
   "source": [
    "image_paths = []\n",
    "labels = []\n",
    "valid_suffixes = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".tif\", \".tiff\", \".png\"}\n",
    "\n",
    "for idx, cls in enumerate(class_names):\n",
    "    for sub in [\"default\", \"real_world\"]:\n",
    "        cls_dir = DATA_DIR / cls / sub\n",
    "        if not cls_dir.exists():\n",
    "            continue  # skip if missing\n",
    "        print(\"Looking into:\", cls_dir)\n",
    "        for p in cls_dir.glob(\"*\"):\n",
    "            if p.suffix.lower() in valid_suffixes:\n",
    "                image_paths.append(str(p))\n",
    "                labels.append(idx)\n",
    "\n",
    "print(\"Total images:\", len(image_paths))\n",
    "if len(image_paths) == 0:\n",
    "    raise RuntimeError(\"No images found (check file suffixes and DATA_DIR).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "294c0565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/vaibav/Downloads/Material_Dataset/images/images/aerosol_cans/default/Image_15.png', '/Users/vaibav/Downloads/Material_Dataset/images/images/aerosol_cans/default/Image_29.png', '/Users/vaibav/Downloads/Material_Dataset/images/images/aerosol_cans/default/Image_178.png', '/Users/vaibav/Downloads/Material_Dataset/images/images/aerosol_cans/default/Image_144.png', '/Users/vaibav/Downloads/Material_Dataset/images/images/aerosol_cans/default/Image_150.png'] [0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(image_paths[:5], labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9fe17204",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
    "    image_paths, labels, test_size=0.3, stratify=labels, random_state=SEED\n",
    ")\n",
    "val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
    "    temp_paths, temp_labels, test_size=0.5, stratify=temp_labels, random_state=SEED\n",
    ")\n",
    "\n",
    "# Dataset builder\n",
    "def process_path(path, label):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_image(img, channels=3, expand_animations=False)  # force 3 channels\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    img = tf.keras.applications.efficientnet.preprocess_input(img)\n",
    "    return img, label\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def make_ds(paths, labels, shuffle=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    \n",
    "    def load_and_preprocess(path, label):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_png(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "        \n",
    "        if shuffle:\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "            img = tf.image.random_brightness(img, 0.2)\n",
    "            img = tf.image.random_contrast(img, 0.8, 1.2)\n",
    "        \n",
    "        return img, label\n",
    "    \n",
    "    ds = ds.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=1024)\n",
    "    \n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "# Example usage\n",
    "train_ds = make_ds(train_paths, train_labels, shuffle=True)\n",
    "val_ds   = make_ds(val_paths, val_labels)\n",
    "test_ds  = make_ds(test_paths, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8d92883c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch image shape: (32, 224, 224, 3)\n",
      "Batch dtype: <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "for imgs, labs in train_ds.take(1):\n",
    "    print(\"Batch image shape:\", imgs.shape)   \n",
    "    print(\"Batch dtype:\", imgs.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1101dfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 231ms/step - accuracy: 0.6494 - loss: 1.1882 - val_accuracy: 0.0484 - val_loss: 16.6648\n",
      "Epoch 2/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 222ms/step - accuracy: 0.7740 - loss: 0.7003 - val_accuracy: 0.2716 - val_loss: 8.1912\n",
      "Epoch 3/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 218ms/step - accuracy: 0.8266 - loss: 0.5268 - val_accuracy: 0.1369 - val_loss: 12.8110\n",
      "Epoch 4/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 228ms/step - accuracy: 0.8501 - loss: 0.4430 - val_accuracy: 0.1400 - val_loss: 11.5455\n",
      "Epoch 5/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 217ms/step - accuracy: 0.8691 - loss: 0.3790 - val_accuracy: 0.2409 - val_loss: 9.0565\n",
      "Epoch 6/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 224ms/step - accuracy: 0.8853 - loss: 0.3208 - val_accuracy: 0.1947 - val_loss: 8.5210\n",
      "Epoch 7/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 217ms/step - accuracy: 0.8985 - loss: 0.2856 - val_accuracy: 0.3329 - val_loss: 4.5632\n",
      "Epoch 8/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 214ms/step - accuracy: 0.9040 - loss: 0.2661 - val_accuracy: 0.2089 - val_loss: 7.2008\n",
      "Epoch 9/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 215ms/step - accuracy: 0.9095 - loss: 0.2477 - val_accuracy: 0.4409 - val_loss: 3.8148\n",
      "Epoch 10/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 219ms/step - accuracy: 0.9106 - loss: 0.2528 - val_accuracy: 0.3480 - val_loss: 4.6030\n",
      "Epoch 11/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 218ms/step - accuracy: 0.9264 - loss: 0.2020 - val_accuracy: 0.5978 - val_loss: 2.0228\n",
      "Epoch 12/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 217ms/step - accuracy: 0.9309 - loss: 0.1895 - val_accuracy: 0.6444 - val_loss: 1.7828\n",
      "Epoch 13/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 219ms/step - accuracy: 0.9313 - loss: 0.1867 - val_accuracy: 0.4391 - val_loss: 5.2400\n",
      "Epoch 14/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 224ms/step - accuracy: 0.9208 - loss: 0.2132 - val_accuracy: 0.3324 - val_loss: 6.9243\n",
      "Epoch 15/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 224ms/step - accuracy: 0.9295 - loss: 0.1988 - val_accuracy: 0.6236 - val_loss: 2.9337\n",
      "Epoch 16/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 228ms/step - accuracy: 0.9408 - loss: 0.1604 - val_accuracy: 0.6716 - val_loss: 2.2479\n",
      "Epoch 17/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 215ms/step - accuracy: 0.9351 - loss: 0.1720 - val_accuracy: 0.4964 - val_loss: 3.7293\n",
      "Epoch 18/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 218ms/step - accuracy: 0.9332 - loss: 0.1866 - val_accuracy: 0.6440 - val_loss: 2.1070\n",
      "Epoch 19/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 218ms/step - accuracy: 0.9397 - loss: 0.1623 - val_accuracy: 0.6276 - val_loss: 2.5588\n",
      "Epoch 20/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 214ms/step - accuracy: 0.9469 - loss: 0.1396 - val_accuracy: 0.6467 - val_loss: 3.1039\n",
      "Epoch 21/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 216ms/step - accuracy: 0.9540 - loss: 0.1178 - val_accuracy: 0.7293 - val_loss: 1.7631\n",
      "Epoch 22/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 212ms/step - accuracy: 0.9495 - loss: 0.1277 - val_accuracy: 0.7396 - val_loss: 1.5716\n",
      "Epoch 23/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 213ms/step - accuracy: 0.9418 - loss: 0.1519 - val_accuracy: 0.5893 - val_loss: 2.3112\n",
      "Epoch 24/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 212ms/step - accuracy: 0.9404 - loss: 0.1579 - val_accuracy: 0.6720 - val_loss: 2.6279\n",
      "Epoch 25/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 214ms/step - accuracy: 0.9459 - loss: 0.1495 - val_accuracy: 0.7040 - val_loss: 1.8928\n",
      "Epoch 26/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 207ms/step - accuracy: 0.9478 - loss: 0.1410 - val_accuracy: 0.7427 - val_loss: 1.4858\n",
      "Epoch 27/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 210ms/step - accuracy: 0.9503 - loss: 0.1343 - val_accuracy: 0.7151 - val_loss: 2.1796\n",
      "Epoch 28/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 214ms/step - accuracy: 0.9496 - loss: 0.1349 - val_accuracy: 0.7782 - val_loss: 1.2885\n",
      "Epoch 29/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 212ms/step - accuracy: 0.9580 - loss: 0.1063 - val_accuracy: 0.7751 - val_loss: 1.4511\n",
      "Epoch 30/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 214ms/step - accuracy: 0.9543 - loss: 0.1139 - val_accuracy: 0.7769 - val_loss: 1.2977\n",
      "Epoch 31/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 215ms/step - accuracy: 0.9572 - loss: 0.1092 - val_accuracy: 0.7627 - val_loss: 1.4554\n",
      "Epoch 32/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 215ms/step - accuracy: 0.9529 - loss: 0.1245 - val_accuracy: 0.6796 - val_loss: 2.3964\n",
      "Epoch 33/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 215ms/step - accuracy: 0.9560 - loss: 0.1115 - val_accuracy: 0.7689 - val_loss: 1.7853\n",
      "Epoch 34/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 214ms/step - accuracy: 0.9537 - loss: 0.1176 - val_accuracy: 0.8022 - val_loss: 1.3488\n",
      "Epoch 35/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 213ms/step - accuracy: 0.9569 - loss: 0.1038 - val_accuracy: 0.8089 - val_loss: 1.0908\n",
      "Epoch 36/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 209ms/step - accuracy: 0.9610 - loss: 0.0914 - val_accuracy: 0.7938 - val_loss: 1.3863\n",
      "Epoch 37/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 208ms/step - accuracy: 0.9622 - loss: 0.0930 - val_accuracy: 0.7813 - val_loss: 1.0974\n",
      "Epoch 38/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 211ms/step - accuracy: 0.9595 - loss: 0.1000 - val_accuracy: 0.6782 - val_loss: 2.2925\n",
      "Epoch 39/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 213ms/step - accuracy: 0.9637 - loss: 0.0887 - val_accuracy: 0.8004 - val_loss: 1.4685\n",
      "Epoch 40/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 210ms/step - accuracy: 0.9570 - loss: 0.1150 - val_accuracy: 0.7204 - val_loss: 1.9592\n",
      "Epoch 41/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 213ms/step - accuracy: 0.9537 - loss: 0.1149 - val_accuracy: 0.7667 - val_loss: 1.7885\n",
      "Epoch 42/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 216ms/step - accuracy: 0.9586 - loss: 0.0933 - val_accuracy: 0.8076 - val_loss: 1.4949\n",
      "Epoch 43/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 212ms/step - accuracy: 0.9647 - loss: 0.0828 - val_accuracy: 0.8000 - val_loss: 1.2619\n",
      "Epoch 44/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 213ms/step - accuracy: 0.9653 - loss: 0.0834 - val_accuracy: 0.7938 - val_loss: 1.6313\n",
      "Epoch 45/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 211ms/step - accuracy: 0.9615 - loss: 0.0915 - val_accuracy: 0.7911 - val_loss: 1.4681\n",
      "Epoch 46/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 215ms/step - accuracy: 0.9563 - loss: 0.1052 - val_accuracy: 0.8004 - val_loss: 1.4531\n",
      "Epoch 47/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 213ms/step - accuracy: 0.9584 - loss: 0.1063 - val_accuracy: 0.8124 - val_loss: 1.1168\n",
      "Epoch 48/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 215ms/step - accuracy: 0.9580 - loss: 0.1034 - val_accuracy: 0.7987 - val_loss: 1.3255\n",
      "Epoch 49/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 210ms/step - accuracy: 0.9665 - loss: 0.0744 - val_accuracy: 0.8253 - val_loss: 1.2190\n",
      "Epoch 50/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 210ms/step - accuracy: 0.9653 - loss: 0.0833 - val_accuracy: 0.7947 - val_loss: 1.5330\n",
      "Epoch 51/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 208ms/step - accuracy: 0.9642 - loss: 0.0877 - val_accuracy: 0.7582 - val_loss: 1.9433\n",
      "Epoch 52/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 209ms/step - accuracy: 0.9622 - loss: 0.0911 - val_accuracy: 0.7822 - val_loss: 1.5798\n",
      "Epoch 53/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 209ms/step - accuracy: 0.9645 - loss: 0.0774 - val_accuracy: 0.7569 - val_loss: 1.9612\n",
      "Epoch 54/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 215ms/step - accuracy: 0.9660 - loss: 0.0799 - val_accuracy: 0.8124 - val_loss: 1.2405\n",
      "Epoch 55/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 213ms/step - accuracy: 0.9631 - loss: 0.0830 - val_accuracy: 0.8062 - val_loss: 1.4830\n",
      "Epoch 56/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 212ms/step - accuracy: 0.9607 - loss: 0.0949 - val_accuracy: 0.8071 - val_loss: 1.4421\n",
      "Epoch 57/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 214ms/step - accuracy: 0.9669 - loss: 0.0801 - val_accuracy: 0.8053 - val_loss: 1.8332\n",
      "Epoch 58/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 218ms/step - accuracy: 0.9657 - loss: 0.0766 - val_accuracy: 0.8098 - val_loss: 1.5205\n",
      "Epoch 59/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 215ms/step - accuracy: 0.9647 - loss: 0.0766 - val_accuracy: 0.8204 - val_loss: 1.2385\n",
      "Epoch 60/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 219ms/step - accuracy: 0.9667 - loss: 0.0657 - val_accuracy: 0.8227 - val_loss: 1.0389\n",
      "Epoch 61/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 221ms/step - accuracy: 0.9645 - loss: 0.0782 - val_accuracy: 0.8062 - val_loss: 1.3428\n",
      "Epoch 62/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 218ms/step - accuracy: 0.9643 - loss: 0.0866 - val_accuracy: 0.7738 - val_loss: 1.6430\n",
      "Epoch 63/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 219ms/step - accuracy: 0.9629 - loss: 0.0841 - val_accuracy: 0.7889 - val_loss: 1.2295\n",
      "Epoch 64/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 226ms/step - accuracy: 0.9652 - loss: 0.0802 - val_accuracy: 0.7880 - val_loss: 1.2478\n",
      "Epoch 65/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 218ms/step - accuracy: 0.9625 - loss: 0.0823 - val_accuracy: 0.8049 - val_loss: 1.0944\n",
      "Epoch 66/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 221ms/step - accuracy: 0.9610 - loss: 0.0979 - val_accuracy: 0.7960 - val_loss: 1.2610\n",
      "Epoch 67/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 225ms/step - accuracy: 0.9685 - loss: 0.0677 - val_accuracy: 0.8280 - val_loss: 1.1308\n",
      "Epoch 68/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 238ms/step - accuracy: 0.9715 - loss: 0.0591 - val_accuracy: 0.8209 - val_loss: 1.0765\n",
      "Epoch 69/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 231ms/step - accuracy: 0.9646 - loss: 0.0837 - val_accuracy: 0.7911 - val_loss: 1.3971\n",
      "Epoch 70/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 234ms/step - accuracy: 0.9678 - loss: 0.0696 - val_accuracy: 0.8098 - val_loss: 1.3195\n",
      "Epoch 71/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 226ms/step - accuracy: 0.9696 - loss: 0.0634 - val_accuracy: 0.8182 - val_loss: 1.1890\n",
      "Epoch 72/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 231ms/step - accuracy: 0.9570 - loss: 0.1011 - val_accuracy: 0.7920 - val_loss: 1.6306\n",
      "Epoch 73/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 233ms/step - accuracy: 0.9672 - loss: 0.0690 - val_accuracy: 0.8156 - val_loss: 1.3224\n",
      "Epoch 74/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 236ms/step - accuracy: 0.9679 - loss: 0.0705 - val_accuracy: 0.7809 - val_loss: 1.8732\n",
      "Epoch 75/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 240ms/step - accuracy: 0.9692 - loss: 0.0660 - val_accuracy: 0.7853 - val_loss: 1.6493\n",
      "Epoch 76/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 232ms/step - accuracy: 0.9711 - loss: 0.0657 - val_accuracy: 0.8098 - val_loss: 1.2509\n",
      "Epoch 77/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 238ms/step - accuracy: 0.9688 - loss: 0.0653 - val_accuracy: 0.8271 - val_loss: 1.3343\n",
      "Epoch 78/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 238ms/step - accuracy: 0.9695 - loss: 0.0628 - val_accuracy: 0.7960 - val_loss: 1.4046\n",
      "Epoch 79/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 233ms/step - accuracy: 0.9640 - loss: 0.0837 - val_accuracy: 0.7853 - val_loss: 1.3139\n",
      "Epoch 80/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 228ms/step - accuracy: 0.9614 - loss: 0.0847 - val_accuracy: 0.8169 - val_loss: 1.1220\n",
      "Epoch 81/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 227ms/step - accuracy: 0.9710 - loss: 0.0579 - val_accuracy: 0.8276 - val_loss: 1.2145\n",
      "Epoch 82/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 227ms/step - accuracy: 0.9685 - loss: 0.0630 - val_accuracy: 0.8227 - val_loss: 1.2968\n",
      "Epoch 83/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 229ms/step - accuracy: 0.9710 - loss: 0.0629 - val_accuracy: 0.8284 - val_loss: 1.5356\n",
      "Epoch 84/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 233ms/step - accuracy: 0.9709 - loss: 0.0646 - val_accuracy: 0.8262 - val_loss: 1.4118\n",
      "Epoch 85/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 228ms/step - accuracy: 0.9698 - loss: 0.0660 - val_accuracy: 0.8142 - val_loss: 1.2663\n",
      "Epoch 86/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 224ms/step - accuracy: 0.9700 - loss: 0.0600 - val_accuracy: 0.8329 - val_loss: 1.1442\n",
      "Epoch 87/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 219ms/step - accuracy: 0.9675 - loss: 0.0730 - val_accuracy: 0.8249 - val_loss: 1.3974\n",
      "Epoch 88/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 218ms/step - accuracy: 0.9673 - loss: 0.0772 - val_accuracy: 0.8267 - val_loss: 1.4238\n",
      "Epoch 89/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 241ms/step - accuracy: 0.9702 - loss: 0.0612 - val_accuracy: 0.8231 - val_loss: 1.2434\n",
      "Epoch 90/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 247ms/step - accuracy: 0.9719 - loss: 0.0622 - val_accuracy: 0.8151 - val_loss: 1.3934\n",
      "Epoch 91/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 247ms/step - accuracy: 0.9643 - loss: 0.0769 - val_accuracy: 0.8124 - val_loss: 1.2687\n",
      "Epoch 92/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 229ms/step - accuracy: 0.9712 - loss: 0.0621 - val_accuracy: 0.8293 - val_loss: 1.0191\n",
      "Epoch 93/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 233ms/step - accuracy: 0.9702 - loss: 0.0617 - val_accuracy: 0.8258 - val_loss: 1.2200\n",
      "Epoch 94/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 244ms/step - accuracy: 0.9684 - loss: 0.0663 - val_accuracy: 0.8093 - val_loss: 1.3545\n",
      "Epoch 95/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 231ms/step - accuracy: 0.9694 - loss: 0.0703 - val_accuracy: 0.8173 - val_loss: 1.2771\n",
      "Epoch 96/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 227ms/step - accuracy: 0.9673 - loss: 0.0738 - val_accuracy: 0.8182 - val_loss: 1.4322\n",
      "Epoch 97/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 226ms/step - accuracy: 0.9707 - loss: 0.0615 - val_accuracy: 0.8298 - val_loss: 1.3343\n",
      "Epoch 98/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 231ms/step - accuracy: 0.9709 - loss: 0.0616 - val_accuracy: 0.8320 - val_loss: 1.2852\n",
      "Epoch 99/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 235ms/step - accuracy: 0.9707 - loss: 0.0582 - val_accuracy: 0.8409 - val_loss: 1.4067\n",
      "Epoch 100/100\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 237ms/step - accuracy: 0.9718 - loss: 0.0596 - val_accuracy: 0.8293 - val_loss: 1.1119\n",
      "\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 134ms/step - accuracy: 0.8244 - loss: 1.0950\n",
      "🧪 Test Accuracy: 0.8244\n"
     ]
    }
   ],
   "source": [
    "base = tf.keras.applications.MobileNetV2(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    pooling=\"avg\"\n",
    ")\n",
    "\n",
    "base.trainable = True\n",
    "\n",
    "# Optionally, freeze first few layers and train only the later ones\n",
    "for layer in base.layers[:100]:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "x = tf.keras.layers.Dropout(0.3)(base.output)\n",
    "out = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(base.input, out)\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Train\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=100)\n",
    "\n",
    "# Evaluate\n",
    "loss, acc = model.evaluate(test_ds)\n",
    "print(f\"🧪 Test Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "45b86d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"material_detection.h5\")  # saves as HDF5 file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54d6a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
